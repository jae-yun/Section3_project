{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#출처: https://towardsdatascience.com/implementing-neural-machine-translation-using-keras-8312e4844eb8"
      ],
      "metadata": {
        "id": "HEoYRz_gMvPt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IZ7JqY-mv24r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlL8GIeZKbgb",
        "outputId": "2e1ab203-1081-4b41-b136-e2d84909ca01"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lang = 'deu'\n",
        "#lang = 'jpn'\n",
        "#lang = 'kor'"
      ],
      "metadata": {
        "id": "kDHYF3rh0tkd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the data txt file on disk.\n",
        "\n",
        "data_path = \"/content/drive/My Drive/Colab Notebooks/\"+lang+\"-eng/\"+lang+\".txt\"\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/\"+lang+\"-eng\"\n",
        "print(data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXYpnF-80DEd",
        "outputId": "d4173f73-b6ff-4bba-ad8b-6eda36167803"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/deu-eng/deu.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "lines= pd.read_table(data_path,  names =['source', 'target', 'comments'])\n",
        "#printing sample data from lines\n",
        "lines.sample(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "nCAp383WKFez",
        "outputId": "2d6aab00-399f-4b30-a044-9d23564a083b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  source  \\\n",
              "156207  Tom rides his bicycle every day.   \n",
              "35876                We don't know that.   \n",
              "121353      You can't force Tom to stay.   \n",
              "109646       Tom received an invitation.   \n",
              "16138                   We took showers.   \n",
              "130001     We set a trap to catch a fox.   \n",
              "\n",
              "                                                   target  \\\n",
              "156207               Tom fährt jeden Tag mit dem Fahrrad.   \n",
              "35876                               Wir wissen das nicht.   \n",
              "121353            Du kannst Tom nicht zwingen zu bleiben.   \n",
              "109646                          Tom bekam eine Einladung.   \n",
              "16138                                       Wir duschten.   \n",
              "130001  Wir haben eine Falle gelegt, um einen Fuchs zu...   \n",
              "\n",
              "                                                 comments  \n",
              "156207  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
              "35876   CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "121353  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "109646  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "16138   CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
              "130001  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-395222fc-2980-4c2f-b5bf-ca3a19d4b21f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>156207</th>\n",
              "      <td>Tom rides his bicycle every day.</td>\n",
              "      <td>Tom fährt jeden Tag mit dem Fahrrad.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35876</th>\n",
              "      <td>We don't know that.</td>\n",
              "      <td>Wir wissen das nicht.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121353</th>\n",
              "      <td>You can't force Tom to stay.</td>\n",
              "      <td>Du kannst Tom nicht zwingen zu bleiben.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109646</th>\n",
              "      <td>Tom received an invitation.</td>\n",
              "      <td>Tom bekam eine Einladung.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16138</th>\n",
              "      <td>We took showers.</td>\n",
              "      <td>Wir duschten.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130001</th>\n",
              "      <td>We set a trap to catch a fox.</td>\n",
              "      <td>Wir haben eine Falle gelegt, um einen Fuchs zu...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-395222fc-2980-4c2f-b5bf-ca3a19d4b21f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-395222fc-2980-4c2f-b5bf-ca3a19d4b21f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-395222fc-2980-4c2f-b5bf-ca3a19d4b21f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert source and target text to Lowercase \n",
        "lines.source=lines.source.apply(lambda x: x.lower())\n",
        "lines.target=lines.target.apply(lambda x: x.lower())\n",
        "# Remove quotes from source and target text\n",
        "lines.source=lines.source.apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines.target=lines.target.apply(lambda x: re.sub(\"'\", '', x))\n",
        "# create a set of all special characters\n",
        "special_characters= set(string.punctuation)\n",
        "# Remove all the special characters\n",
        "lines.source = lines.source.apply(lambda x: ''.join(char1 for char1 in x if char1 not in special_characters))\n",
        "lines.target = lines.target.apply(lambda x: ''.join(char1 for char1 in x if char1 not in special_characters))\n",
        "# Remove digits from source and target sentences\n",
        "num_digits= str.maketrans('','', digits)\n",
        "lines.source=lines.source.apply(lambda x: x.translate(num_digits))\n",
        "lines.target= lines.target.apply(lambda x: x.translate(num_digits))\n",
        "# Remove extra spaces\n",
        "lines.source=lines.source.apply(lambda x: x.strip())\n",
        "lines.target=lines.target.apply(lambda x: x.strip())\n",
        "lines.source=lines.source.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines.target=lines.target.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "\n",
        "print('lines.shape:',lines.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PocyL29vLUaY",
        "outputId": "edbb0f7e-8d51-4214-da29-708aee2336b7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lines.shape: (260434, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter()\n",
        "\n",
        "def word_count(docs):\n",
        "   \n",
        "    word_counts = Counter()\n",
        "\n",
        "    # 단어가 존재하는 문서의 빈도 카운트, 단어가 한 번 이상 존재하면 +1\n",
        "    word_in_docs = Counter()\n",
        "\n",
        "    # 전체 문서의 갯수\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    for doc in docs:\n",
        "        # 문서 내에 단어를 모두 카운트 \n",
        "        # ex) 'I studied, studied and studied.' -> ('I', 1), ('studied', 3), ('and', 1)\n",
        "        word_counts.update(doc)\n",
        "        # set -> 중복된 요소는 포함하지 않음\n",
        "        # 문서 내에 단어가 한 번 이상 등장했는지 카운트 ex) 'I studied, studied and studied.' -> ('I', 1), ('studied', 1), ('and', 1)\n",
        "        word_in_docs.update(set(doc))\n",
        "    \n",
        "    # 단어와 카운트 횟수로 dataframe을 만들어주기 위해 묶어줍니다\n",
        "    temp = zip(word_counts.keys(), word_counts.values())\n",
        "\n",
        "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "    # 단어의 순위\n",
        "    # method='first': 같은 값의 경우 먼저 나온 요소를 우선\n",
        "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "    # 전체 문서에 등장한 모든 단어 수\n",
        "    total = wc['count'].sum()\n",
        "\n",
        "    # 코퍼스 내 단어의 비율\n",
        "    wc['percent'] = wc['count'].apply(lambda x: x / total)\n",
        "\n",
        "    #단어 누적 합을 계산할 때, 많이 등장한 단어부터 더해주기 위해 정렬\n",
        "    wc = wc.sort_values(by='rank')\n",
        "\n",
        "    # 누적 비율\n",
        "    # cumsum() : cumulative sum\n",
        "    wc['cul_percent'] = wc['percent'].cumsum()\n",
        "\n",
        "    temp2 = zip(word_in_docs.keys(), word_in_docs.values())\n",
        "    #'word_in_docs' : 해당 단어를 포함하고 있는 문서(doc)의 수\n",
        "    ac = pd.DataFrame(temp2, columns=['word', 'word_in_docs'])\n",
        "    wc = ac.merge(wc, on='word')\n",
        "    \n",
        "    # 전체 문서 중 존재하는 비율\n",
        "    wc['word_in_docs_percent'] = wc['word_in_docs'].apply(lambda x: x / total_docs)\n",
        "\n",
        "    return wc.sort_values(by='rank')\n",
        "\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(lines['source']):\n",
        "    #각 문서의 토큰마다 소문자 변환 및 정규표현식(영소문자 및 영어를 제외하고 모두 제거)를 적용\n",
        "    doc_tokens = [re.sub(r\"[^a-z0-9]\", \"\", token.text.lower()) for token in doc]\n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "wc = word_count(tokens)\n",
        "\n",
        "print('wc.shape:',wc.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y51GK4JYejiV",
        "outputId": "f629beb0-ed59-432e-f1ed-00375291bdd4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wc.shape: (17260, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.lineplot(x='rank', y='cul_percent', data=wc);\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "8Cu6BXZatnmP",
        "outputId": "792c24cb-047b-45b4-986b-70327035603f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wc[wc['rank'] <= 1000]['cul_percent'].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBwnTXR_trL6",
        "outputId": "dacaa920-76e9-43e5-cc74-dc054cdbdb72"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8849984473347489"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wc[wc['rank'] <= 2000]['cul_percent'].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJxfPEsDTC9j",
        "outputId": "0935b958-061f-4c5e-e147-b3f8340d00b6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9374236250371116"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wc[wc['rank'] <= 5000]['cul_percent'].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBJIif6jS_Yr",
        "outputId": "3e568bda-869c-4f73-d98b-47cc656efb2d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9788819224007949"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wc[wc['rank'] <= 10000]['cul_percent'].max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3TpEWXlTAS7",
        "outputId": "6297e315-fe46-47fd-8b35-786dafcd2f10"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9942743562537507"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wc2 = wc[wc['word_in_docs_percent'] <= 0.01]\n",
        "wc2 = wc[wc['rank'] >= 5000]\n",
        "\n",
        "\n",
        "def word2idx(sent):\n",
        "    sent_token = sent.split()\n",
        "    return [1 if word in sent_token else 0 for word in wc2['word']]\n",
        "\n",
        "def word2idx2(sente):\n",
        "    return sum(word2idx(sente))\n",
        "\n",
        "lines['tag'] = lines['source'].apply(word2idx2)\n",
        "lines2 = lines[lines['tag'] == 0]\n",
        "\n",
        "\n",
        "print('lines.shape:',lines2.shape)\n",
        "print('wc.shape:', wc2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0PQ89IfhgsK",
        "outputId": "e80f59bd-ac52-4f70-fb0e-ab1bcf3844f6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lines.shape: (230907, 4)\n",
            "wc.shape: (12261, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines2.to_csv(file_path+\"/lines2.csv\")"
      ],
      "metadata": {
        "id": "CSUTyABwv1NG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = file_path+\"/lines2.csv\"\n",
        "\n",
        "lines2= pd.read_csv(data_path, index_col=0, names=['ID', 'source', 'target', 'comments', 'tag'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HAYoz7bv3Tf",
        "outputId": "76a23872-4dd3-4180-9e7c-149500e4dfbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lines2.sample(frac=0.5, random_state=42)\n",
        "lines2 = lines2.sample(n=80000, random_state=42)\n",
        "\n",
        "print('lines.shape:',lines2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hN_nK8V5yN2",
        "outputId": "5ef0cbf6-3e33-40e6-bc90-ea1cd69b5d9e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lines.shape: (80000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add start and end tokens to target sequences\n",
        "lines2.target = lines2.target.apply(lambda x : 'START_ '+ x + ' _END')\n",
        "lines2.sample(6)\n",
        "\n",
        "\n",
        "\n",
        "# Find all the source and target words and sort them\n",
        "# Vocabulary of Source language\n",
        "all_source_words=set()\n",
        "for source in lines2.source:\n",
        "    for word in source.split():\n",
        "        if word not in all_source_words:\n",
        "            all_source_words.add(word)\n",
        "# Vocabulary of Target \n",
        "all_target_words=set()\n",
        "for target in lines2.target:\n",
        "    for word in target.split():\n",
        "        if word not in all_target_words:\n",
        "            all_target_words.add(word)\n",
        "# sort all unique source and target words\n",
        "source_words= sorted(list(all_source_words))\n",
        "target_words=sorted(list(all_target_words))\n",
        "\n",
        "#Find maximum sentence length in  the source and target data\n",
        "source_length_list=[]\n",
        "for l in lines2.source:\n",
        "    source_length_list.append(len(l.split(' ')))\n",
        "max_source_length= max(source_length_list)\n",
        "print(\" Max length of the source sentence\",max_source_length)\n",
        "target_length_list=[]\n",
        "for l in lines2.target:\n",
        "    target_length_list.append(len(l.split(' ')))\n",
        "max_target_length= max(target_length_list)\n",
        "print(\" Max length of the target sentence\",max_target_length)\n",
        "\n",
        "# creating a word to index(word2idx) for source and target\n",
        "source_word2idx= dict([(word, i+1) for i,word in enumerate(source_words)])\n",
        "target_word2idx=dict([(word, i+1) for i, word in enumerate(target_words)])\n",
        "\n",
        "#creating a dictionary for index to word for source and target vocabulary\n",
        "source_idx2word= dict([(i, word) for word, i in  source_word2idx.items()])\n",
        "print(source_idx2word)\n",
        "target_idx2word =dict([(i, word) for word, i in target_word2idx.items()])\n",
        "\n",
        "#Shuffle the data\n",
        "lines2 = shuffle(lines2)\n",
        "\n",
        "# Train - Test Split\n",
        "X, y = lines2.source, lines2.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "################\n",
        "\n",
        "# Input tokens for encoder\n",
        "num_encoder_tokens=len(source_words)\n",
        "# Input tokens for decoder zero padded\n",
        "num_decoder_tokens=len(target_words) +1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFjDhFWmeiy8",
        "outputId": "4ecc9def-b120-4470-f35b-4fb6988d5487"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Max length of the source sentence 25\n",
            " Max length of the target sentence 27\n",
            "{1: 'a', 2: 'able', 3: 'about', 4: 'abroad', 5: 'accept', 6: 'accident', 7: 'across', 8: 'actually', 9: 'address', 10: 'advice', 11: 'afford', 12: 'afraid', 13: 'after', 14: 'afternoon', 15: 'again', 16: 'against', 17: 'age', 18: 'ago', 19: 'agree', 20: 'air', 21: 'airport', 22: 'alice', 23: 'alive', 24: 'all', 25: 'allowed', 26: 'almost', 27: 'alone', 28: 'along', 29: 'already', 30: 'also', 31: 'always', 32: 'am', 33: 'an', 34: 'and', 35: 'angry', 36: 'animals', 37: 'another', 38: 'answer', 39: 'answered', 40: 'any', 41: 'anybody', 42: 'anymore', 43: 'anyone', 44: 'anything', 45: 'anywhere', 46: 'apartment', 47: 'apologize', 48: 'apple', 49: 'apples', 50: 'are', 51: 'arent', 52: 'arm', 53: 'around', 54: 'arrested', 55: 'arrive', 56: 'arrived', 57: 'as', 58: 'ask', 59: 'asked', 60: 'asking', 61: 'asleep', 62: 'at', 63: 'ate', 64: 'attention', 65: 'audiobook\\xad', 66: 'australia', 67: 'away', 68: 'baby', 69: 'back', 70: 'bad', 71: 'bag', 72: 'ball', 73: 'bank', 74: 'baseball', 75: 'be', 76: 'beach', 77: 'beautiful', 78: 'became', 79: 'because', 80: 'become', 81: 'bed', 82: 'been', 83: 'beer', 84: 'before', 85: 'began', 86: 'beginning', 87: 'behind', 88: 'being', 89: 'believe', 90: 'best', 91: 'better', 92: 'between', 93: 'bicycle', 94: 'big', 95: 'bike', 96: 'birthday', 97: 'bit', 98: 'black', 99: 'blame', 100: 'blue', 101: 'book', 102: 'books', 103: 'born', 104: 'boss', 105: 'boston', 106: 'both', 107: 'bottle', 108: 'bought', 109: 'box', 110: 'boy', 111: 'boyfriend', 112: 'boys', 113: 'bread', 114: 'break', 115: 'breakfast', 116: 'bridge', 117: 'bring', 118: 'broke', 119: 'broken', 120: 'brother', 121: 'brothers', 122: 'brought', 123: 'building', 124: 'bus', 125: 'business', 126: 'busy', 127: 'but', 128: 'buy', 129: 'by', 130: 'café', 131: 'cafés', 132: 'cake', 133: 'call', 134: 'called', 135: 'came', 136: 'camera', 137: 'can', 138: 'canadian', 139: 'cannot', 140: 'cant', 141: 'car', 142: 'care', 143: 'careful', 144: 'carefully', 145: 'carry', 146: 'cars', 147: 'case', 148: 'cat', 149: 'catch', 150: 'cats', 151: 'caught', 152: 'chair', 153: 'chance', 154: 'change', 155: 'changed', 156: 'check', 157: 'child', 158: 'children', 159: 'choice', 160: 'christmas', 161: 'church', 162: 'city', 163: 'class', 164: 'clean', 165: 'clear', 166: 'close', 167: 'closed', 168: 'clothes', 169: 'coat', 170: 'coffee', 171: 'cold', 172: 'college', 173: 'color', 174: 'come', 175: 'comes', 176: 'coming', 177: 'common', 178: 'company', 179: 'completely', 180: 'computer', 181: 'concert', 182: 'cook', 183: 'cooking', 184: 'correct', 185: 'cost', 186: 'could', 187: 'couldnt', 188: 'country', 189: 'couple', 190: 'crazy', 191: 'cry', 192: 'crying', 193: 'cup', 194: 'cut', 195: 'dance', 196: 'danger', 197: 'dangerous', 198: 'dark', 199: 'daughter', 200: 'day', 201: 'days', 202: 'dead', 203: 'deal', 204: 'death', 205: 'decided', 206: 'decision', 207: 'desk', 208: 'dictionary', 209: 'did', 210: 'didnt', 211: 'die', 212: 'died', 213: 'difference', 214: 'different', 215: 'difficult', 216: 'dinner', 217: 'disappointed', 218: 'do', 219: 'doctor', 220: 'does', 221: 'doesnt', 222: 'doesn’t', 223: 'dog', 224: 'dogs', 225: 'dog\\u200b', 226: 'doing', 227: 'dollars', 228: 'done', 229: 'dont', 230: 'don’t', 231: 'door', 232: 'doubt', 233: 'down', 234: 'dream', 235: 'dress', 236: 'drink', 237: 'drinking', 238: 'drive', 239: 'driver', 240: 'driving', 241: 'drunk', 242: 'during', 243: 'each', 244: 'earlier', 245: 'early', 246: 'easily', 247: 'easy', 248: 'eat', 249: 'eaten', 250: 'eating', 251: 'eats', 252: 'eggs', 253: 'either', 254: 'else', 255: 'empty', 256: 'end', 257: 'english', 258: 'enjoy', 259: 'enjoyed', 260: 'enough', 261: 'entrée', 262: 'even', 263: 'evening', 264: 'ever', 265: 'every', 266: 'everybody', 267: 'everyone', 268: 'everything', 269: 'exactly', 270: 'exam', 271: 'excuse', 272: 'expect', 273: 'expected', 274: 'expensive', 275: 'explain', 276: 'eye', 277: 'eyes', 278: 'face', 279: 'family', 280: 'famous', 281: 'far', 282: 'fast', 283: 'father', 284: 'fault', 285: 'favorite', 286: 'feel', 287: 'feeling', 288: 'fell', 289: 'felt', 290: 'few', 291: 'fiancé', 292: 'fiancée', 293: 'fight', 294: 'finally', 295: 'find', 296: 'fine', 297: 'finish', 298: 'finished', 299: 'fire', 300: 'fired', 301: 'first', 302: 'fish', 303: 'fishing', 304: 'five', 305: 'fix', 306: 'floor', 307: 'flowers', 308: 'follow', 309: 'food', 310: 'for', 311: 'forget', 312: 'forgot', 313: 'forward', 314: 'found', 315: 'four', 316: 'free', 317: 'french', 318: 'friend', 319: 'friends', 320: 'from', 321: 'front', 322: 'full', 323: 'fun', 324: 'funny', 325: 'future', 326: 'game', 327: 'garden', 328: 'gave', 329: 'get', 330: 'gets', 331: 'getting', 332: 'girl', 333: 'girlfriend', 334: 'girls', 335: 'give', 336: 'given', 337: 'glad', 338: 'glass', 339: 'glasses', 340: 'go', 341: 'goes', 342: 'going', 343: 'gone', 344: 'good', 345: 'got', 346: 'great', 347: 'guess', 348: 'guitar', 349: 'guy', 350: 'guys', 351: 'had', 352: 'hadnt', 353: 'hair', 354: 'half', 355: 'hand', 356: 'hands', 357: 'happen', 358: 'happened', 359: 'happens', 360: 'happy', 361: 'hard', 362: 'harder', 363: 'hardly', 364: 'has', 365: 'hasnt', 366: 'hat', 367: 'hate', 368: 'have', 369: 'havent', 370: 'haven’t', 371: 'having', 372: 'he', 373: 'head', 374: 'health', 375: 'hear', 376: 'heard', 377: 'heart', 378: 'heavy', 379: 'hed', 380: 'hell', 381: 'help', 382: 'helped', 383: 'her', 384: 'here', 385: 'hes', 386: 'high', 387: 'him', 388: 'himself', 389: 'his', 390: 'hit', 391: 'hold', 392: 'home', 393: 'homework', 394: 'honest', 395: 'hope', 396: 'horse', 397: 'hospital', 398: 'hot', 399: 'hotel', 400: 'hour', 401: 'hours', 402: 'house', 403: 'how', 404: 'hundred', 405: 'hungry', 406: 'hurry', 407: 'hurt', 408: 'husband', 409: 'i', 410: 'ice', 411: 'id', 412: 'idea', 413: 'if', 414: 'ill', 415: 'im', 416: 'immediately', 417: 'important', 418: 'impossible', 419: 'in', 420: 'inside', 421: 'interested', 422: 'interesting', 423: 'into', 424: 'invited', 425: 'is', 426: 'isnt', 427: 'it', 428: 'itll', 429: 'its', 430: 'it’s', 431: 'ive', 432: 'i’ll', 433: 'i’m', 434: 'japan', 435: 'japanese', 436: 'job', 437: 'john', 438: 'joke', 439: 'just', 440: 'keep', 441: 'kept', 442: 'key', 443: 'keys', 444: 'kids', 445: 'kill', 446: 'killed', 447: 'kind', 448: 'kiss', 449: 'kissed', 450: 'kitchen', 451: 'knew', 452: 'knife', 453: 'know', 454: 'known', 455: 'knows', 456: 'language', 457: 'languages', 458: 'large', 459: 'last', 460: 'late', 461: 'later', 462: 'laugh', 463: 'laughed', 464: 'lawyer', 465: 'learn', 466: 'learned', 467: 'least', 468: 'leave', 469: 'leaving', 470: 'left', 471: 'lend', 472: 'less', 473: 'let', 474: 'lets', 475: 'letter', 476: 'library', 477: 'lie', 478: 'lied', 479: 'life', 480: 'light', 481: 'like', 482: 'liked', 483: 'likely', 484: 'likes', 485: 'list', 486: 'listen', 487: 'listening', 488: 'little', 489: 'live', 490: 'lived', 491: 'lives', 492: 'living', 493: 'long', 494: 'longer', 495: 'look', 496: 'looked', 497: 'looking', 498: 'looks', 499: 'lose', 500: 'lost', 501: 'lot', 502: 'love', 503: 'loved', 504: 'loves', 505: 'lucky', 506: 'lunch', 507: 'lying', 508: 'machine', 509: 'mad', 510: 'made', 511: 'make', 512: 'makes', 513: 'making', 514: 'man', 515: 'many', 516: 'married', 517: 'mary', 518: 'marys', 519: 'matter', 520: 'may', 521: 'maybe', 522: 'me', 523: 'mean', 524: 'means', 525: 'meat', 526: 'medicine', 527: 'meet', 528: 'meeting', 529: 'men', 530: 'message', 531: 'met', 532: 'might', 533: 'milk', 534: 'mind', 535: 'mine', 536: 'minute', 537: 'minutes', 538: 'miss', 539: 'missed', 540: 'mistake', 541: 'mistakes', 542: 'moment', 543: 'monday', 544: 'money', 545: 'month', 546: 'months', 547: 'mood', 548: 'more', 549: 'morning', 550: 'most', 551: 'mother', 552: 'mountain', 553: 'move', 554: 'moved', 555: 'movie', 556: 'movies', 557: 'much', 558: 'music', 559: 'must', 560: 'my', 561: 'myself', 562: 'name', 563: 'nattō', 564: 'naïve', 565: 'near', 566: 'necessary', 567: 'need', 568: 'needed', 569: 'needs', 570: 'nervous', 571: 'never', 572: 'new', 573: 'news', 574: 'next', 575: 'nice', 576: 'night', 577: 'no', 578: 'nobody', 579: 'noise', 580: 'not', 581: 'nothing', 582: 'notice', 583: 'noticed', 584: 'now', 585: 'number', 586: 'oclock', 587: 'october', 588: 'of', 589: 'off', 590: 'offer', 591: 'office', 592: 'often', 593: 'ok', 594: 'old', 595: 'older', 596: 'on', 597: 'once', 598: 'one', 599: 'ones', 600: 'only', 601: 'open', 602: 'opened', 603: 'opinion', 604: 'or', 605: 'order', 606: 'other', 607: 'others', 608: 'ought', 609: 'our', 610: 'out', 611: 'outside', 612: 'over', 613: 'owe', 614: 'own', 615: 'paid', 616: 'pain', 617: 'paper', 618: 'parents', 619: 'park', 620: 'part', 621: 'party', 622: 'pass', 623: 'passed', 624: 'past', 625: 'pay', 626: 'people', 627: 'perfect', 628: 'person', 629: 'phone', 630: 'piano', 631: 'pick', 632: 'picked', 633: 'picture', 634: 'pictures', 635: 'piece', 636: 'place', 637: 'plan', 638: 'plane', 639: 'planning', 640: 'plans', 641: 'play', 642: 'played', 643: 'player', 644: 'playing', 645: 'plays', 646: 'please', 647: 'point', 648: 'police', 649: 'poor', 650: 'popular', 651: 'possible', 652: 'prefer', 653: 'present', 654: 'pretty', 655: 'price', 656: 'prison', 657: 'probably', 658: 'problem', 659: 'problems', 660: 'promise', 661: 'promised', 662: 'proud', 663: 'put', 664: 'question', 665: 'questions', 666: 'quickly', 667: 'quiet', 668: 'quit', 669: 'quite', 670: 'radio', 671: 'rain', 672: 'raining', 673: 'ran', 674: 'rather', 675: 'raúl', 676: 'read', 677: 'reading', 678: 'ready', 679: 'real', 680: 'really', 681: 'reason', 682: 'recently', 683: 'red', 684: 'remember', 685: 'report', 686: 'rest', 687: 'restaurant', 688: 'return', 689: 'rice', 690: 'rich', 691: 'right', 692: 'river', 693: 'road', 694: 'room', 695: 'rules', 696: 'run', 697: 'running', 698: 'sad', 699: 'safe', 700: 'said', 701: 'same', 702: 'sat', 703: 'save', 704: 'saw', 705: 'say', 706: 'saying', 707: 'says', 708: 'scared', 709: 'school', 710: 'seat', 711: 'second', 712: 'secret', 713: 'see', 714: 'seeing', 715: 'seem', 716: 'seemed', 717: 'seems', 718: 'seen', 719: 'seldom', 720: 'sell', 721: 'send', 722: 'sense', 723: 'sent', 724: 'serious', 725: 'set', 726: 'several', 727: 'she', 728: 'shes', 729: 'ship', 730: 'shirt', 731: 'shoes', 732: 'shopping', 733: 'short', 734: 'shot', 735: 'should', 736: 'shouldnt', 737: 'shouldve', 738: 'show', 739: 'showed', 740: 'shower', 741: 'shut', 742: 'sick', 743: 'side', 744: 'sign', 745: 'since', 746: 'sing', 747: 'singing', 748: 'sister', 749: 'sisters', 750: 'sit', 751: 'sitting', 752: 'situation', 753: 'six', 754: 'sky', 755: 'sleep', 756: 'sleeping', 757: 'slept', 758: 'small', 759: 'smile', 760: 'smoke', 761: 'smoking', 762: 'snow', 763: 'so', 764: 'soccer', 765: 'solve', 766: 'some', 767: 'somebody', 768: 'someone', 769: 'something', 770: 'sometimes', 771: 'somewhere', 772: 'son', 773: 'song', 774: 'soon', 775: 'sorry', 776: 'sound', 777: 'sounds', 778: 'speak', 779: 'speaking', 780: 'speaks', 781: 'spend', 782: 'spent', 783: 'spoke', 784: 'stand', 785: 'standing', 786: 'start', 787: 'started', 788: 'station', 789: 'stay', 790: 'stayed', 791: 'staying', 792: 'still', 793: 'stolen', 794: 'stood', 795: 'stop', 796: 'stopped', 797: 'store', 798: 'story', 799: 'strange', 800: 'street', 801: 'strong', 802: 'student', 803: 'students', 804: 'study', 805: 'studying', 806: 'stupid', 807: 'success', 808: 'such', 809: 'suddenly', 810: 'summer', 811: 'sun', 812: 'sunday', 813: 'supposed', 814: 'sure', 815: 'surprised', 816: 'swim', 817: 'swimming', 818: 'table', 819: 'take', 820: 'taken', 821: 'takes', 822: 'taking', 823: 'talk', 824: 'talked', 825: 'talking', 826: 'tall', 827: 'taught', 828: 'taxi', 829: 'tea', 830: 'teach', 831: 'teacher', 832: 'team', 833: 'telephone', 834: 'television', 835: 'tell', 836: 'telling', 837: 'ten', 838: 'tennis', 839: 'terrible', 840: 'test', 841: 'than', 842: 'thank', 843: 'thanks', 844: 'that', 845: 'thats', 846: 'the', 847: 'their', 848: 'them', 849: 'then', 850: 'there', 851: 'theres', 852: 'there’s', 853: 'these', 854: 'they', 855: 'theyre', 856: 'thing', 857: 'things', 858: 'think', 859: 'thinking', 860: 'thinks', 861: 'thirty', 862: 'this', 863: 'those', 864: 'though', 865: 'thought', 866: 'three', 867: 'through', 868: 'ticket', 869: 'tie', 870: 'time', 871: 'times', 872: 'tired', 873: 'to', 874: 'today', 875: 'together', 876: 'tokyo', 877: 'told', 878: 'tom', 879: 'tomorrow', 880: 'toms', 881: 'tonight', 882: 'too', 883: 'took', 884: 'top', 885: 'touch', 886: 'town', 887: 'traffic', 888: 'train', 889: 'translate', 890: 'travel', 891: 'tree', 892: 'tried', 893: 'trip', 894: 'trouble', 895: 'true', 896: 'trust', 897: 'truth', 898: 'try', 899: 'trying', 900: 'turn', 901: 'turned', 902: 'tv', 903: 'two', 904: 'umbrella', 905: 'under', 906: 'understand', 907: 'until', 908: 'up', 909: 'upset', 910: 'us', 911: 'use', 912: 'used', 913: 'usually', 914: 'vacation', 915: 'very', 916: 'visit', 917: 'visited', 918: 'voice', 919: 'wait', 920: 'waited', 921: 'waiting', 922: 'wake', 923: 'walk', 924: 'walked', 925: 'walking', 926: 'wall', 927: 'want', 928: 'wanted', 929: 'wants', 930: 'war', 931: 'warm', 932: 'was', 933: 'wash', 934: 'wasnt', 935: 'watch', 936: 'watching', 937: 'water', 938: 'way', 939: 'we', 940: 'wear', 941: 'wearing', 942: 'wears', 943: 'weather', 944: 'week', 945: 'weekend', 946: 'weeks', 947: 'weight', 948: 'welcome', 949: 'well', 950: 'went', 951: 'were', 952: 'werent', 953: 'weve', 954: 'what', 955: 'whatever', 956: 'whats', 957: 'when', 958: 'where', 959: 'wheres', 960: 'whether', 961: 'which', 962: 'while', 963: 'white', 964: 'who', 965: 'whole', 966: 'whos', 967: 'whose', 968: 'why', 969: 'wife', 970: 'will', 971: 'win', 972: 'window', 973: 'wine', 974: 'winter', 975: 'wish', 976: 'with', 977: 'without', 978: 'woman', 979: 'women', 980: 'won', 981: 'wonder', 982: 'wont', 983: 'won’t', 984: 'word', 985: 'words', 986: 'work', 987: 'worked', 988: 'working', 989: 'works', 990: 'world', 991: 'worried', 992: 'worry', 993: 'worse', 994: 'worth', 995: 'would', 996: 'wouldnt', 997: 'wouldve', 998: 'write', 999: 'writing', 1000: 'written', 1001: 'wrong', 1002: 'wrote', 1003: 'year', 1004: 'years', 1005: 'yes', 1006: 'yesterday', 1007: 'yet', 1008: 'you', 1009: 'youd', 1010: 'youll', 1011: 'young', 1012: 'younger', 1013: 'your', 1014: 'youre', 1015: 'yours', 1016: 'yourself', 1017: 'youve', 1018: 'you’ll', 1019: 'you’re', 1020: '€'}\n",
            "(118559,) (13174,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_source_length),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_target_length),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_target_length, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                  encoder_input_data[i, t] = source_word2idx[word] \n",
        "                  for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_word2idx[word] # decoder input seq\n",
        "                        if t>0:\n",
        "                                          # decoder target sequence (one hot encoded)\n",
        "                                          # does not include the START_ token\n",
        "                                          # Offset by one timestep\n",
        "                                          #print(word)\n",
        "                            decoder_target_data[i, t - 1, target_word2idx[word]] = 1.\n",
        "                    \n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "metadata": {
        "id": "8zVXqd3wL4Ka"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "latent_dim=256"
      ],
      "metadata": {
        "id": "KEZX9QB-L8Sc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "####\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "####\n",
        "\n",
        "# Define the model that takes encoder and decoder input \n",
        "# to output decoder_outputs\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "h5N9Tqs7L_Ph"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "vwvm53-AMIba"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = len(X_train) # Total Training samples\n",
        "val_samples = len(X_test)    # Total validation or test samples\n",
        "batch_size = 128\n",
        "epochs = 100\n"
      ],
      "metadata": {
        "id": "Kgcr3godMLTb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 저장할 폴더와 형식을 선택\n",
        "checkPoint_path = file_path+\"/model_{epoch}.ckpt\" # 저장할 당시 epoch가 파일이름이 된다.\n",
        "\n",
        "# 2. 콜백 변수를 생성\n",
        "my_period = train_samples//batch_size\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkPoint_path,\n",
        "\t\t\t\t\tsave_weights_only=True, verbose=1, save_freq=my_period)\n",
        "\n",
        "print(my_period)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeIQ-2EMVK-y",
        "outputId": "5743d50b-78a6-4d58-a05e-bbac3e62ada6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 체크포인트들이 있는 폴더 선택\n",
        "checkPoint_dir = os.path.dirname(file_path+\"/model_{epoch}.ckpt\")\n",
        "\n",
        "# 2. 해당 폴더에서 가장 마지막 체크포인트 선택\n",
        "latest = tf.train.latest_checkpoint(checkPoint_dir)\n",
        "\n",
        "# 3. 해당체크포인트에 저장한 모델의 가중치 불러오기\n",
        "model.load_weights(latest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOuzrbB1VSNL",
        "outputId": "461a8c20-3f53-4aa5-bf71-b37aa80250f4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fda004ad7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size,\n",
        "                    callbacks=[cp_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0R3k-ORSMN2b",
        "outputId": "6d6321eb-0c97-47fe-9fcc-f1f88cbd6788"
      },
      "execution_count": 31,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-109a3284168e>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 5.6861 - acc: 0.0541\n",
            "Epoch 1: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_1.ckpt\n",
            "562/562 [==============================] - 373s 643ms/step - loss: 5.6854 - acc: 0.0541 - val_loss: 5.3918 - val_acc: 0.0584\n",
            "Epoch 2/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 5.2863 - acc: 0.0750\n",
            "Epoch 2: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_2.ckpt\n",
            "562/562 [==============================] - 345s 614ms/step - loss: 5.2859 - acc: 0.0751 - val_loss: 5.1813 - val_acc: 0.0986\n",
            "Epoch 3/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 5.0504 - acc: 0.1213\n",
            "Epoch 3: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_3.ckpt\n",
            "562/562 [==============================] - 351s 624ms/step - loss: 5.0502 - acc: 0.1214 - val_loss: 4.9414 - val_acc: 0.1356\n",
            "Epoch 4/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 4.8436 - acc: 0.1464\n",
            "Epoch 4: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_4.ckpt\n",
            "562/562 [==============================] - 352s 626ms/step - loss: 4.8432 - acc: 0.1465 - val_loss: 4.7501 - val_acc: 0.1571\n",
            "Epoch 5/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 4.6608 - acc: 0.1662\n",
            "Epoch 5: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_5.ckpt\n",
            "562/562 [==============================] - 351s 625ms/step - loss: 4.6606 - acc: 0.1662 - val_loss: 4.5884 - val_acc: 0.1742\n",
            "Epoch 6/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 4.4997 - acc: 0.1830\n",
            "Epoch 6: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_6.ckpt\n",
            "562/562 [==============================] - 353s 629ms/step - loss: 4.4996 - acc: 0.1830 - val_loss: 4.4636 - val_acc: 0.1905\n",
            "Epoch 7/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 4.3474 - acc: 0.2002\n",
            "Epoch 7: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_7.ckpt\n",
            "562/562 [==============================] - 348s 620ms/step - loss: 4.3471 - acc: 0.2002 - val_loss: 4.2959 - val_acc: 0.2113\n",
            "Epoch 8/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 4.2057 - acc: 0.2170\n",
            "Epoch 8: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_8.ckpt\n",
            "562/562 [==============================] - 356s 634ms/step - loss: 4.2058 - acc: 0.2170 - val_loss: 4.1732 - val_acc: 0.2262\n",
            "Epoch 9/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 4.0683 - acc: 0.2340\n",
            "Epoch 9: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_9.ckpt\n",
            "562/562 [==============================] - 358s 638ms/step - loss: 4.0683 - acc: 0.2340 - val_loss: 4.0503 - val_acc: 0.2394\n",
            "Epoch 10/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.9393 - acc: 0.2500\n",
            "Epoch 10: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_10.ckpt\n",
            "562/562 [==============================] - 357s 636ms/step - loss: 3.9391 - acc: 0.2500 - val_loss: 3.9302 - val_acc: 0.2557\n",
            "Epoch 11/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.8209 - acc: 0.2644\n",
            "Epoch 11: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_11.ckpt\n",
            "562/562 [==============================] - 354s 631ms/step - loss: 3.8210 - acc: 0.2644 - val_loss: 3.8193 - val_acc: 0.2677\n",
            "Epoch 12/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.7075 - acc: 0.2780\n",
            "Epoch 12: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_12.ckpt\n",
            "562/562 [==============================] - 349s 622ms/step - loss: 3.7075 - acc: 0.2780 - val_loss: 3.7291 - val_acc: 0.2805\n",
            "Epoch 13/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.6005 - acc: 0.2917\n",
            "Epoch 13: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_13.ckpt\n",
            "562/562 [==============================] - 357s 636ms/step - loss: 3.6007 - acc: 0.2917 - val_loss: 3.6328 - val_acc: 0.2938\n",
            "Epoch 14/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.5031 - acc: 0.3038\n",
            "Epoch 14: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_14.ckpt\n",
            "562/562 [==============================] - 358s 636ms/step - loss: 3.5029 - acc: 0.3038 - val_loss: 3.5729 - val_acc: 0.3012\n",
            "Epoch 15/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.4137 - acc: 0.3151\n",
            "Epoch 15: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_15.ckpt\n",
            "562/562 [==============================] - 358s 637ms/step - loss: 3.4133 - acc: 0.3151 - val_loss: 3.4771 - val_acc: 0.3113\n",
            "Epoch 16/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.3300 - acc: 0.3257\n",
            "Epoch 16: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_16.ckpt\n",
            "562/562 [==============================] - 350s 623ms/step - loss: 3.3300 - acc: 0.3257 - val_loss: 3.4892 - val_acc: 0.3136\n",
            "Epoch 17/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.2513 - acc: 0.3354\n",
            "Epoch 17: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_17.ckpt\n",
            "562/562 [==============================] - 358s 638ms/step - loss: 3.2514 - acc: 0.3353 - val_loss: 3.3438 - val_acc: 0.3277\n",
            "Epoch 18/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.1799 - acc: 0.3446\n",
            "Epoch 18: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_18.ckpt\n",
            "562/562 [==============================] - 350s 622ms/step - loss: 3.1795 - acc: 0.3447 - val_loss: 3.2904 - val_acc: 0.3352\n",
            "Epoch 19/100\n",
            "561/562 [============================>.] - ETA: 0s - loss: 3.1108 - acc: 0.3532\n",
            "Epoch 19: saving model to /content/drive/My Drive/Colab Notebooks/deu-eng/model_19.ckpt\n",
            "562/562 [==============================] - 349s 621ms/step - loss: 3.1110 - acc: 0.3532 - val_loss: 3.2476 - val_acc: 0.3400\n",
            "Epoch 20/100\n",
            "477/562 [========================>.....] - ETA: 47s - loss: 3.0520 - acc: 0.3609"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-109a3284168e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2602\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         )\n\u001b[0;32m-> 2604\u001b[0;31m         return self.fit(\n\u001b[0m\u001b[1;32m   2605\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(file_path+'/nmt_weights_5000_words.h5')"
      ],
      "metadata": {
        "id": "4uGdS9ItMQOx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    steps = val_samples//batch_size,\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3zb3kKLnUYu",
        "outputId": "fe5ff812-8c14-42d5-9032-683fca3c256e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 28s 211ms/step - loss: 0.9495 - acc: 0.6664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9495357275009155, 0.6663960218429565]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(file_path+'/nmt_weights_1000_words.h5')\n",
        "#model.load_weights(file_path+'/nmt_weights_10000_samples.h5')\n",
        "#model.load_weights(file_path+'/nmt_weights_5000_words.h5')\n"
      ],
      "metadata": {
        "id": "Cw2Q5bc_MSW_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the input sequence to get the \"Context vectors\"\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_state_input,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "8tV6AhdSMWPq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of \n",
        "    #target sequence with the start character.\n",
        "    target_seq[0, 0] = target_word2idx['START_']\n",
        "# Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "# Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word =target_idx2word[sampled_token_index]\n",
        "        decoded_sentence += ' '+ sampled_word\n",
        "# Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_word == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "# Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "# Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "GkrnqBFLMYiz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=-1"
      ],
      "metadata": {
        "id": "-uAG7U5vMbEx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input Source sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Target Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Target Translation:', decoded_sentence[:-4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzb0IubqMm1q",
        "outputId": "3a48e090-669d-4349-e9bf-ecbc5dd24155"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "Input Source sentence: tom went there in person\n",
            "Actual Target Translation:  tom ging persönlich hin \n",
            "Predicted Target Translation:  tom ging persönlich hin gehen haben haben haben h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
        "k=10\n",
        "k+=1\n",
        "(input_seq, actual_output), _ = next(test_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input Source sentence:', X_test[k:k+1].values[0])\n",
        "print('Actual Target Translation:', y_test[k:k+1].values[0][6:-4])\n",
        "print('Predicted Target Translation:', decoded_sentence[:-4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu8LOsHVMo9E",
        "outputId": "e86f7d77-3f16-4edd-c990-2dbd974eb230"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Input Source sentence: i read a book as i walked\n",
            "Actual Target Translation:  ich las ein buch als ich spazieren ging \n",
            "Predicted Target Translation:  tom ist nicht sehr groß aus haben haben haben h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#언어, 문장수, 단어수, 에폭을 기준으로 성능측정.\n",
        "#이중번역 성능측정.\n"
      ],
      "metadata": {
        "id": "a1vxsZrS6s9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aset에 대한 결과\n",
        "\n",
        "Input Source sentence: i do remember\n",
        "Actual Target Translation:  ich erinnere mich \n",
        "Predicted Target Translation:  ich bin nicht daran gewöhnt dass die zukunft eine nachr\n",
        "\n",
        "Input Source sentence: tom went there in person\n",
        "Actual Target Translation:  tom ging persönlich hin \n",
        "Predicted Target Translation:  tom ging persönlich hin gehen haben haben haben h\n",
        "\n",
        "\n",
        "Input Source sentence: youre not supposed to be in here\n",
        "Actual Target Translation:  sie sollten eigentlich nicht hier drinnen sein \n",
        "Predicted Target Translation:  dir kam gestern im park sehen zu sein haben haben h\n",
        "\n",
        "Input Source sentence: i read a book as i walked\n",
        "Actual Target Translation:  ich las ein buch als ich spazieren ging \n",
        "Predicted Target Translation:  tom ist nicht sehr groß aus haben haben haben h\n",
        "\n",
        "\n",
        " loss: 0.9495 - acc: 0.6664"
      ],
      "metadata": {
        "id": "dW8mpA2AkcZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R7uFm8XBQfcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ArVCDC6JQfaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RPMEa9UdQfV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FjZhwSDEQfLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Cset 에 대한 결과\n",
        "\n",
        "\n",
        "Input Source sentence: tom is likely to get married again\n",
        "Actual Target Translation:  tom wird wahrscheinlich wieder heiraten \n",
        "Predicted Target Translation:  tom wird wahrscheinlich wieder heiraten werden zu \n",
        "\n",
        "\n",
        "\n",
        "Input Source sentence: whats your favorite kind of sushi\n",
        "Actual Target Translation:  was ist dein lieblingssushi \n",
        "Predicted Target Translation:  wir müssen tom aufhalten bevor er sich nicht g\n",
        "\n",
        "\n",
        "loss: 1.2561 - acc: 0.6408\n"
      ],
      "metadata": {
        "id": "RK9xVOu3koT6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}